{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d5dcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import h2o\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d03a6",
   "metadata": {},
   "source": [
    "# Prediction testing\n",
    "I will test whether the model can be used to predict given an input or not. First of all, I will do the testing in my local jupyter notebook. This means, I will load the model, preprocess the input data, and predict the input data. Then, I will create a FastAPI app for this model, and try to replicate the prediction using the API. \n",
    "\n",
    "## Local notebook prediction\n",
    "First, I will get the model path and initialize H2O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf081f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "model_filename = 'model/dl_grid_model_66'\n",
    "knn_initial_filename = 'model/knn_imputer_model.pkl'\n",
    "knn_cur_filename = 'model/knn_imputer_model_no_multicol.pkl'\n",
    "scaler_filename = 'model/scaler_no_multicol.pkl'\n",
    "#years_in_current_job_filename = 'model/years_in_current_job_mapping.pkl'\n",
    "purpose_filename = 'model/purpose_mapping.pkl'\n",
    "\n",
    "model_path = os.path.join(current_dir, model_filename)\n",
    "knn_initial_path = os.path.join(current_dir, knn_initial_filename)\n",
    "knn_cur_path = os.path.join(current_dir, knn_cur_filename)\n",
    "scaler_path = os.path.join(current_dir, scaler_filename)\n",
    "#years_in_current_job_path = os.path.join(current_dir, years_in_current_job_filename)\n",
    "purpose_path = os.path.join(current_dir, purpose_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4498b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM (build 17.0.8+9-LTS-211, mixed mode, sharing)\n",
      "  Starting server from C:\\Users\\agust\\Anaconda3\\envs\\h2o_loan_classification\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\agust\\AppData\\Local\\Temp\\tmpym8ubcvs\n",
      "  JVM stdout: C:\\Users\\agust\\AppData\\Local\\Temp\\tmpym8ubcvs\\h2o_agust_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\agust\\AppData\\Local\\Temp\\tmpym8ubcvs\\h2o_agust_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Asia/Jakarta</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.42.0.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>1 month and 22 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_agust_aktxgq</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>3.854 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.11.4 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         01 secs\n",
       "H2O_cluster_timezone:       Asia/Jakarta\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.42.0.2\n",
       "H2O_cluster_version_age:    1 month and 22 days\n",
       "H2O_cluster_name:           H2O_from_python_agust_aktxgq\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    3.854 Gb\n",
       "H2O_cluster_total_cores:    16\n",
       "H2O_cluster_allowed_cores:  16\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.11.4 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1818e",
   "metadata": {},
   "source": [
    "Then, I will load all the models or saved dictionaries used in preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4441eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = h2o.load_model(model_path)\n",
    "knn_initial_model = joblib.load(knn_initial_path)\n",
    "knn_cur_model = joblib.load(knn_cur_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "#years_in_current_job_mapping = joblib.load(years_in_current_job_path)\n",
    "purpose_mapping = joblib.load(purpose_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6904b",
   "metadata": {},
   "source": [
    "I will create a dictionary as an input to the model. This is just to simplify the process. Usually, the software developer will create a front end app to get the input in a predetermined format, and I will get the data from the API created using requests library. The data is usually in a JSON format, and I have to convert it into python dictionary. But for this purpose, let's just say that the dictionary below is created from JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23308494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test dictionary\n",
    "test_dict = {'current_loan_amount': 10167,\n",
    " 'term': 'Short Term',\n",
    " 'credit_score': 7380.0,\n",
    " 'years_in_current_job': 3,\n",
    " 'home_ownership': 'Own Home',\n",
    " 'annual_income': 42701.0,\n",
    " 'purpose': 'Debt Consolidation',\n",
    " 'monthly_debt': 761.51,\n",
    " 'years_of_credit_history': 25.8,\n",
    " 'months_since_last_delinquent': 5.5,\n",
    " 'number_of_open_accounts': 7,\n",
    " 'number_of_credit_problems': 0,\n",
    " 'current_credit_balance': 11283,\n",
    " 'maximum_open_credit': 16954.0,\n",
    " 'bankruptcies': 0.0,\n",
    " 'tax_liens': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485d7f4",
   "metadata": {},
   "source": [
    "Next, I will create a function with the aim of processing the dictionary and turn it into a working dataframe, with the exact same columns as the one used in the model training process. The steps are:\n",
    "1. Converting the dictionary into pandas dataframe.\n",
    "2. Clean credit_score column. The score above 850 will be divided by 10.\n",
    "3. Convert the values in the term column into 1 for long term and 0 for short term.\n",
    "4. Impute the missing columns using the saved KNN model from the previous notebook.\n",
    "5. Simplify purpose into debt_consolidation, business_loans, personal_loans, and other.\n",
    "6. Create the features from the previous notebook, then drop the unused columns.\n",
    "7. Impute credit_utilization_ratio column if necessary.\n",
    "8. One hot encode the purpose and home_ownership columns, assign it to the correct dummy variables, and drop the original columns.\n",
    "9. Standardize the numerical values except for the ratio and binary columns.\n",
    "10. Return the resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920a75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(data, knn_initial_model, purpose_mapping, knn_cur_model, scaler):\n",
    "    df = pd.DataFrame([data])\n",
    "        \n",
    "    #clean credit score\n",
    "    df.loc[df['credit_score'] > 850, 'credit_score'] = df.loc[df['credit_score'] > 850, 'credit_score'] / 10\n",
    "    \n",
    "    #clean home ownership\n",
    "    df['home_ownership'] = df['home_ownership'].replace('HaveMortgage', 'Home Mortgage')\n",
    "    \n",
    "    #convert string values into lower case and snake case\n",
    "    df = df.applymap(lambda x: x if not isinstance(x, str) or not helper.has_non_ascii(x) else x.encode('ascii', 'ignore').decode('ascii'))\n",
    "    categorical_cols = ['term', 'home_ownership', 'purpose']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = helper.clean_columns(df[col].tolist())\n",
    "    \n",
    "    #convert term \n",
    "    term_dict = {'short_term':0, 'long_term':1}\n",
    "    df.replace({\"term\": term_dict}, inplace=True)\n",
    "    \n",
    "    #impute missing values if any\n",
    "    column_names_to_impute = ['current_loan_amount', 'credit_score', 'years_in_current_job', 'annual_income', 'months_since_last_delinquent', 'maximum_open_credit', 'bankruptcies', 'tax_liens']\n",
    "    column_with_missing_values = df.columns[df.isnull().any()].tolist()\n",
    "    imputed = knn_initial_model.transform(df[column_names_to_impute].values)\n",
    "    data_temp = pd.DataFrame(imputed, columns=column_names_to_impute, index = df.index)\n",
    "    df[column_with_missing_values] = data_temp[column_with_missing_values]\n",
    "    \n",
    "    #simplify purpose\n",
    "    df['purpose'] = df['purpose'].map(purpose_mapping)\n",
    "    df['purpose'].fillna('other', inplace=True)\n",
    "    \n",
    "    #feature engineering\n",
    "    df['debt_equity_ratio'] = df['monthly_debt'] / df['annual_income']\n",
    "    df['credit_utilization_ratio'] = df['current_credit_balance'] / df['maximum_open_credit']\n",
    "    df['is_months_delinquent_missing'] = df['months_since_last_delinquent'].isnull().astype(int)\n",
    "    df['has_stable_job'] = (df['years_in_current_job'] > 2).astype(int)\n",
    "    \n",
    "    #drop unneeded columns\n",
    "    df.drop(['bankruptcies', 'monthly_debt', 'annual_income', 'current_credit_balance', 'years_in_current_job', 'maximum_open_credit', 'months_since_last_delinquent', 'tax_liens'], axis = 1, inplace = True)\n",
    "    \n",
    "    #impute credit_utilization_ratio if needed\n",
    "    column_with_missing_values = df.columns[df.isnull().any()].tolist()\n",
    "    if len(column_with_missing_values) > 0:\n",
    "        column_names_to_impute = ['credit_utilization_ratio']\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        imputed = knn_cur_model.transform(df[column_names_to_impute].values)\n",
    "        data_temp = pd.DataFrame(imputed, columns=column_names_to_impute, index = df.index)\n",
    "        df[column_names_to_impute] = data_temp\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #one hot encode purpose and home_ownership\n",
    "    #dummy variable names for purpose and home_ownership in the expected dataframe \n",
    "    all_purpose_cols = ['purpose_debt_consolidation', 'purpose_other', 'purpose_personal_loans'] \n",
    "    all_home_cols = ['home_own_home', 'home_rent']\n",
    "    \n",
    "    #one hot encode\n",
    "    new_dummies_purpose = pd.get_dummies(df['purpose'], prefix='purpose').replace({True: 1, False: 0})\n",
    "    new_dummies_home = pd.get_dummies(df['home_ownership'], prefix='home').replace({True: 1, False: 0})\n",
    "    list_dummies_purpose = list(new_dummies_purpose.columns)\n",
    "    list_dummies_home = list(new_dummies_home.columns)\n",
    "    \n",
    "    #create similar column to expected dataframe\n",
    "    for col in all_purpose_cols:\n",
    "        if col not in new_dummies_purpose.columns:\n",
    "            new_dummies_purpose[col] = 0\n",
    "    for col in all_home_cols:\n",
    "        if col not in new_dummies_home.columns:\n",
    "            new_dummies_home[col] = 0\n",
    "    \n",
    "    #drop first dummies if neccessary\n",
    "    for col in list_dummies_purpose:\n",
    "        if col in all_purpose_cols:\n",
    "            pass\n",
    "        else:\n",
    "            new_dummies_purpose.drop(col, axis=1,inplace=True)\n",
    "\n",
    "    for col in list_dummies_home:\n",
    "        if col in list(all_home_cols):\n",
    "            pass\n",
    "        else:\n",
    "            new_dummies_home.drop(col, axis=1,inplace=True)\n",
    "    \n",
    "    #change the values into the dummy variables\n",
    "    df[new_dummies_purpose.columns] = new_dummies_purpose\n",
    "    df[new_dummies_home.columns] = new_dummies_home\n",
    "    \n",
    "    #drop home_ownership and purpose\n",
    "    df.drop([\"home_ownership\", \"purpose\"], axis=1, inplace = True)\n",
    "    \n",
    "    #standardize numeric values except for binaries and ratios\n",
    "    cols_to_standardize = ['current_loan_amount', 'credit_score', 'years_of_credit_history', 'number_of_open_accounts', 'number_of_credit_problems']\n",
    "    data_scaled = scaler.transform(df[cols_to_standardize].values)\n",
    "    data_temp = pd.DataFrame(data_scaled, columns=cols_to_standardize, index = df.index)\n",
    "    df[cols_to_standardize] = data_temp\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0eb7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the function to preprocess the input data\n",
    "df = create_dataframe(test_dict, knn_initial_model, purpose_mapping, knn_cur_model, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d288982a",
   "metadata": {},
   "source": [
    "Next, I will create a prediction function. The function will return a dictionary of the prediction. The function takes an H2O model, H2O dataframe, and threshold_metrics.\n",
    "\n",
    "The reason I use threshold_metrics is to give flexibility in predicting the data for the users. I can change it into precision, recall, or f1 if necessary. The steps to create the function is as follows:\n",
    "1. Predict the H2O dataframe using the model\n",
    "2. Convert the prediction result to pandas dataframe\n",
    "3. Get the probability of loan_given\n",
    "4. Find the threshold by maximum metric (precision, recall, or f1)\n",
    "5. Compare the probability of loan given to the threshold. If it's higher, then the loan is given. If it's lower, then the loan is refused.\n",
    "6. Create a dictionary consisting of a list of the resulting prediction.\n",
    "7. Return the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa464bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(model, hf, threshold_metrics='precision'):\n",
    "    #predict\n",
    "    predictions = model.predict(hf)\n",
    "    \n",
    "    #convert to pandas dataframe\n",
    "    prediction_df = predictions.as_data_frame()\n",
    "    \n",
    "    #applying threshold to the prediction\n",
    "    loan_given_prob = prediction_df['loan_given'].tolist()[0]\n",
    "    #loan_refused_prob = prediction_df['loan_refused'].tolist()[0]\n",
    "    threshold = model.find_threshold_by_max_metric(threshold_metrics)\n",
    "    \n",
    "    loan_prediction = [('loan_given' if loan_given_prob > threshold else 'loan_refused').replace('_', ' ').title()]\n",
    "    \n",
    "    #create prediction dictionary for json\n",
    "    prediction_dict = {}\n",
    "    prediction_dict['prediction'] = loan_prediction\n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d042d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "#parse the pandas dataframe into H2O dataframe\n",
    "hf = h2o.H2OFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89fe4139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning prediction progress: |██████████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': ['Loan Given']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict the data\n",
    "predict_data(model, hf, threshold_metrics='precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe101e",
   "metadata": {},
   "source": [
    "## FastAPI Prediction\n",
    "I deployed the model in the 'app' folder. The functions from the helper function is selected, and I saved those which are useful for prediction into preprocessing.py.\n",
    "\n",
    "To run the server, go to terminal and type:\n",
    "uvicorn main:LoanPredApp --reload\n",
    "\n",
    "'main' is used because the file is called main.py\n",
    "'LoanPredApp' is used because the app inside the main.py is called LoanPredApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66a3eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "948d8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://localhost:8000/predict'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e70854",
   "metadata": {},
   "source": [
    "To call the API, we use requests.post with the format below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac54838",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    url,\n",
    "    json=test_dict,  # Serialize the input_dict as JSON\n",
    "    headers={\"Content-Type\": \"application/json\"}  # Set the appropriate Content-Type header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02a2dd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': ['Loan Given']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d61afe8",
   "metadata": {},
   "source": [
    "For the original data, the prediction is the same, which is 'Loan Given'. I will test it with another data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "159a5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test dictionary\n",
    "test_dict = {'current_loan_amount': 5167,\n",
    " 'term': 'Long Term',\n",
    " 'credit_score': 350.0,\n",
    " 'years_in_current_job': 1,\n",
    " 'home_ownership': 'Rent',\n",
    " 'annual_income': 12701.0,\n",
    " 'purpose': 'Debt Consolidation',\n",
    " 'monthly_debt': 1061.51,\n",
    " 'years_of_credit_history': 25.8,\n",
    " 'months_since_last_delinquent': 5.5,\n",
    " 'number_of_open_accounts': 7,\n",
    " 'number_of_credit_problems': 0,\n",
    " 'current_credit_balance': 11283,\n",
    " 'maximum_open_credit': 16954.0,\n",
    " 'bankruptcies': 0.0,\n",
    " 'tax_liens': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07a4712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    url,\n",
    "    json=test_dict,  # Serialize the input_dict as JSON\n",
    "    headers={\"Content-Type\": \"application/json\"}  # Set the appropriate Content-Type header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "573257db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': ['Loan Refused']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcf480f",
   "metadata": {},
   "source": [
    "And for this data, the prediction is 'Loan Refused'.\n",
    "\n",
    "The next thing to do is dockerize this app, and I can consider this project finished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "H2O Loan Prediction",
   "language": "python",
   "name": "h2o_loan_classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
